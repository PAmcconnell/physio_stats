{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script Name: clean_physio_task-rest_ppg.py\n",
    "\n",
    "Description:\n",
    "This script is designed for processing and analyzing pulse wave (PPG) signals within a BIDS (Brain Imaging Data Structure) dataset. \n",
    "\n",
    "Usage:\n",
    "The script is intended to be run from the command line or an IDE. It requires the specification of dataset directories and participant/session details within the script. The script can be customized to process multiple participants and sessions by modifying the relevant sections.\n",
    "\n",
    "Requirements:\n",
    "- Python 3.x\n",
    "- Libraries: neurokit2, pandas, matplotlib, scipy, numpy\n",
    "- A BIDS-compliant dataset with PPG signal data.\n",
    "\n",
    "Note:\n",
    "- Ensure that the dataset paths and participant/session details are correctly set in the script.\n",
    "- The script contains several hardcoded paths and parameters which may need to be adjusted based on the dataset structure and analysis needs.\n",
    "\n",
    "Author: PAMcConnell\n",
    "Date: 20231215\n",
    "Version: 1.0\n",
    "\n",
    "\"\"\"\n",
    "#%% Import libraries\n",
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import gzip\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Data handling and numerical computation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import iirfilter, sosfreqz, sosfiltfilt, freqz\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.decomposition import FastICA\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import t\n",
    "import dash\n",
    "from dash import Dash, dcc, html, Input, Output, State, callback_context\n",
    "\n",
    "# Performance profiling and resource management\n",
    "import psutil\n",
    "import cProfile\n",
    "import tracemalloc\n",
    "\n",
    "# Neuroimaging data processing\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.interfaces.fsl import MCFLIRT\n",
    "import shutil  # For file operations such as moving or deleting files\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')  # Use 'Agg' backend for file-based plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly # For interactive plotting\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Neurophysiological data analysis\n",
    "import neurokit2 as nk\n",
    "\n",
    "# conda activate nipype (Python 3.9)\n",
    "\n",
    "# Function to log system resource usage\n",
    "def log_resource_usage():\n",
    "    \"\"\"Logs the CPU and Memory usage of the system.\"\"\"\n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)\n",
    "    logging.info(f\"Current memory usage: {memory_usage}%, CPU usage: {cpu_usage}%\")\n",
    "\n",
    "# Sets up archival logging for the script, directing log output to both a file and the console.\n",
    "def setup_logging(subject_id, session_id, run_id, dataset_root_dir):\n",
    "    \"\"\"\n",
    "\n",
    "    The function configures logging to capture informational, warning, and error messages. It creates a unique log file for each \n",
    "    subject-session combination, stored in a 'logs' directory within the 'doc' folder adjacent to the BIDS root directory.\n",
    "\n",
    "  \n",
    "    Parameters:\n",
    "    - subject_id (str): The identifier for the subject.\n",
    "    - session_id (str): The identifier for the session.\n",
    "    - data_root_dir (str): The root directory of the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - log_file_path (str): The path to the log file.\n",
    "\n",
    "    This function sets up a logging system that writes logs to both a file and the console. \n",
    "    The log file is named based on the subject ID, session ID, and the script name. \n",
    "    It's stored in a 'logs' directory within the 'doc' folder by subject ID, which is located at the same \n",
    "    level as the BIDS root directory.\n",
    "\n",
    "    The logging level is set to INFO, meaning it captures all informational, warning, and error messages.\n",
    "\n",
    "    Usage Example:\n",
    "    setup_logging('sub-01', 'ses-1', '/path/to/bids_root_dir')\n",
    "    \"\"\"\n",
    "\n",
    "    try: \n",
    "        # Extract the base name of the script without the .py extension.\n",
    "        script_name = os.path.basename(__file__).replace('.py', '')\n",
    "\n",
    "        # Construct the log directory path within 'doc/logs'\n",
    "        log_dir = os.path.join(os.path.dirname(dataset_root_dir), 'doc', 'logs', script_name, subject_id, run_id)\n",
    "        print(f\"Checking log directory: {log_dir}\")\n",
    "\n",
    "        # Create the log directory if it doesn't exist.\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "        # Construct the log file name using subject ID, session ID, and script name.\n",
    "        log_file_name = f\"{subject_id}_{session_id}_task-rest_{run_id}_{script_name}.log\"\n",
    "        log_file_path = os.path.join(log_dir, log_file_name)\n",
    "\n",
    "        # Configure file logging.\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            filename=log_file_path,\n",
    "            filemode='w' # 'w' mode overwrites existing log file\n",
    "        )\n",
    "\n",
    "        # If you also want to log to console.\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "        logging.info(f\"Logging setup complete. Log file: {log_file_path}\")\n",
    "\n",
    "        return log_file_path\n",
    "    \n",
    "    except Exception as e:\n",
    "            print(f\"Error setting up logging: {e}\")\n",
    "            sys.exit(1) # Exiting the script due to logging setup failure.\n",
    "\n",
    "def comb_band_stop_filter(data, fs, order=4, visualize=False):\n",
    "    \"\"\"\n",
    "    Apply a comb band-stop filter.\n",
    "\n",
    "    Parameters:\n",
    "    - data (array-like or pandas.Series): The input signal.\n",
    "    - stop_freq (float): The stop frequency of the filter (center of the notch).\n",
    "    - fs (int): The sampling rate of the signal.\n",
    "    - order (int, optional): The order of the filter.\n",
    "\n",
    "    Returns:\n",
    "    - y (numpy.ndarray): The filtered signal.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert Pandas Series to NumPy array if necessary\n",
    "    if isinstance(data, pd.Series):\n",
    "        data = data.values\n",
    "\n",
    "    # Ensure data is a 1D array\n",
    "    if data.ndim != 1:\n",
    "        raise ValueError(\"The input data must be a 1D array or Series.\")\n",
    "\n",
    "    # Design the Butterworth filter\n",
    "    nyquist = 0.5 * fs\n",
    "    #nyquist = 11.5 * fs  # 11.5 Hz -> nSlices/(MBF*TR) = 69/(2*3) = 11.5 Hz ? \n",
    "    #normal_stop_freq = stop_freq / nyquist\n",
    "    \n",
    "    # Calculate the stop band frequencies\n",
    "    stop_band = [0.49,0.51]\n",
    "\n",
    "    # Design the bandstop filter\n",
    "    sos = iirfilter(order, stop_band, btype='bandstop', fs=fs, output='sos')\n",
    "\n",
    "    # Visualize the frequency response\n",
    "    if visualize:\n",
    "        w, h = sosfreqz(sos, worN=8000, fs=fs)\n",
    "        plt.plot(w, abs(h), label=\"Frequency Response\")\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Gain')\n",
    "        plt.title('Frequency Response of the Band-Stop Filter')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Apply the filter\n",
    "    y = sosfiltfilt(sos, data)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Function to calculate framewise displacement\n",
    "def calculate_fd(motion_params_file):\n",
    "    \"\"\"\n",
    "    Calculate framewise displacement from motion parameters.\n",
    "\n",
    "    This function reads motion parameters from a file, computes translations and rotations,\n",
    "    and calculates the framewise displacement. The rotations are converted to mm using a \n",
    "    specified radius (default is 50 mm) before summing with translations for FD calculation.\n",
    "\n",
    "    Parameters:\n",
    "    motion_params_file (str): Path to the file containing motion parameters.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Array of framewise displacement values, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load motion parameters\n",
    "        motion_params = np.loadtxt(motion_params_file)\n",
    "\n",
    "        # Calculate translations and rotations\n",
    "        translations = np.abs(np.diff(motion_params[:, :3], axis=0))\n",
    "        rotations = np.abs(np.diff(motion_params[:, 3:], axis=0)) * np.pi / 180 * 50  # Convert to mm\n",
    "\n",
    "        # Sum translations and rotations for FD\n",
    "        fd = np.sum(translations, axis=1) + np.sum(rotations, axis=1)\n",
    "        logging.info(\"Framewise displacement calculated successfully.\")\n",
    "\n",
    "        return fd\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in calculating framewise displacement: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to calculate the correlation between FD and PPG\n",
    "def calculate_fd_ppg_correlation(fd, ppg):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient and p-value between framewise displacement (FD) and PPG.\n",
    "\n",
    "    Parameters:\n",
    "    fd (array-like): The framewise displacement timeseries.\n",
    "    ppg (array-like): The ppg activity timeseries.\n",
    "\n",
    "    Returns:\n",
    "    float: The Pearson correlation coefficient.\n",
    "    float: The p-value indicating statistical significance.\n",
    "\n",
    "    The function assumes both inputs are numpy arrays of the same length.\n",
    "    The Pearson correlation coefficient measures the linear correlation between the two arrays,\n",
    "    returning a value between -1 and 1, where 1 means total positive linear correlation,\n",
    "    0 means no linear correlation, and -1 means total negative linear correlation.\n",
    "    The p-value roughly indicates the probability of an uncorrelated system producing datasets\n",
    "    that have a Pearson correlation at least as extreme as the one computed from these datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure that FD and PPG are of the same length\n",
    "    if len(fd) != len(ppg):\n",
    "        logging.error(\"FD and PPG timeseries must be of the same length.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # Calculate Pearson correlation\n",
    "        r_value, p_value = pearsonr(fd, ppg)\n",
    "        logging.info(f\"Calculated Pearson correlation: {r_value}, p-value: {p_value}\")\n",
    "        \n",
    "        # Round p-value to 3 decimal places\n",
    "        p_value_rounded = round(p_value, 3)\n",
    "        p_value = p_value_rounded\n",
    "\n",
    "        # Check for very large sample sizes which might render p-value as 0\n",
    "        if p_value == 0:\n",
    "            logging.info(\"P-value returned as 0, possibly due to large sample size and high precision of correlation.\")\n",
    "\n",
    "        return r_value, p_value\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in calculating correlation: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to plot the correlation between FD and PPG\n",
    "def plot_fd_ppg_correlation(fd, ppg, file_name):\n",
    "    \n",
    "    if len(fd) != len(ppg):\n",
    "        logging.warning(\"Error: FD and PPG timeseries must be of the same length.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Perform linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(fd, ppg)\n",
    "        fit_line = slope * fd + intercept\n",
    "        r_squared = r_value**2\n",
    "        \n",
    "        # Calculate the confidence interval of the fit line\n",
    "        t_val = t.ppf(1-0.05/2, len(fd)-2)  # t-score for 95% confidence interval & degrees of freedom\n",
    "        conf_interval = t_val * std_err * np.sqrt(1/len(fd) + (fd - np.mean(fd))**2 / np.sum((fd - np.mean(fd))**2))\n",
    "\n",
    "        # Upper and lower bounds of the confidence interval\n",
    "        lower_bound = fit_line - conf_interval\n",
    "        upper_bound = fit_line + conf_interval\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(fd, ppg, alpha=0.5, label='Data Points')\n",
    "        plt.plot(fd, fit_line, color='red', label=f'Fit Line (R = {r_value:.3f}, p = {p_value:.3f})')\n",
    "        plt.fill_between(fd, lower_bound, upper_bound, color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "        plt.ylabel('PPG (Volts)')\n",
    "        plt.xlabel('Framewise Displacement (mm)')\n",
    "        plt.title('Correlation between FD and PPG with Linear Fit and Confidence Interval')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.savefig(file_name, dpi=300)  # Save the figure with increased DPI for higher resolution\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"An error occurred: {e}\")\n",
    "   \n",
    "# Save framewise displacement data to a TSV file\n",
    "def save_fd_to_tsv(fd_timeseries, output_dir, filename):\n",
    "    \"\"\"\n",
    "    Save framewise displacement data to a TSV file.\n",
    "\n",
    "    This function takes framewise displacement data, converts it to a pandas DataFrame, \n",
    "    and then saves it as a tab-separated values (TSV) file in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    fd_timeseries (list or array-like): The framewise displacement timeseries data.\n",
    "    output_dir (str): The directory where the TSV file will be saved.\n",
    "    filename (str): The name of the TSV file.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the file is saved successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Ensuring output directory exists\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            logging.info(f\"Created directory: {output_dir}\")\n",
    "\n",
    "        # Converting the timeseries data to a DataFrame\n",
    "        fd_df = pd.DataFrame(fd_timeseries, columns=['Framewise_Displacement'])\n",
    "\n",
    "        # Constructing the full file path\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Saving the DataFrame to a TSV file\n",
    "        fd_df.to_csv(file_path, sep='\\t', index=False)\n",
    "        logging.info(f\"Framewise displacement data saved to {file_path}\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in saving framewise displacement data: {e}\")\n",
    "        return False\n",
    "\n",
    "# Function to run MCFLIRT motion correction on a 4D fMRI dataset and generate motion parameters\n",
    "def run_mcflirt_motion_correction(original_data_path, output_dir, working_dir):\n",
    "    \"\"\"\n",
    "    Run MCFLIRT motion correction on a 4D fMRI dataset.\n",
    "\n",
    "    This function applies MCFLIRT, an FSL tool for motion correction, to a 4D fMRI dataset. It saves\n",
    "    the motion-corrected output, motion plots, and transformation matrices to specified directories.\n",
    "\n",
    "    Parameters:\n",
    "    original_data_path (str): Path to the original 4D fMRI data file.\n",
    "    output_dir (str): Directory where the motion-corrected file and related outputs will be saved.\n",
    "    working_dir (str): Working directory for intermediate files.\n",
    "\n",
    "    Returns:\n",
    "    str: The path of the motion-corrected output file, or None if an error occurs.\n",
    "\n",
    "    Raises:\n",
    "    Exception: Propagates any exceptions that occur during processing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Running MCFLIRT motion correction...\")\n",
    "\n",
    "        # Configure MCFLIRT\n",
    "        mcflirt = MCFLIRT()\n",
    "        mcflirt.inputs.in_file = original_data_path\n",
    "        out_filename = 'mcf_' + os.path.basename(original_data_path).replace('.gz', '')  # Handles both .nii and .nii.gz\n",
    "        mcflirt.inputs.out_file = os.path.join(output_dir, out_filename)\n",
    "        \n",
    "        logging.info(f\"Output file: {mcflirt.inputs.out_file}\")\n",
    "\n",
    "        # Set MCFLIRT options\n",
    "        mcflirt.inputs.save_plots = True  # Save motion plots\n",
    "        mcflirt.inputs.save_mats = True   # Save transformation matrices\n",
    "        mcflirt.base_dir = working_dir    # Set working directory\n",
    "\n",
    "        # Run MCFLIRT\n",
    "        mcflirt.run()\n",
    "\n",
    "        logging.info(\"MCFLIRT motion correction completed successfully.\")\n",
    "        return mcflirt.inputs.out_file\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during motion correction with MCFLIRT: {e}\")\n",
    "        raise\n",
    "\n",
    "#%% Main script logic \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to clean PPG data from a BIDS dataset.\n",
    "    \"\"\"\n",
    "    print(f\"Starting main function\")\n",
    "    start_time = datetime.now()\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    # Define and check dataset root directory\n",
    "    dataset_root_dir = os.path.expanduser('~/Documents/MRI/LEARN/BIDS_test/')\n",
    "    print(f\"Checking BIDS root directory: {dataset_root_dir}\")\n",
    "    if not os.path.exists(dataset_root_dir):\n",
    "        print(f\"Directory not found: {dataset_root_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Define and check BIDS root directory\n",
    "    bids_root_dir = os.path.expanduser('~/Documents/MRI/LEARN/BIDS_test/dataset/')\n",
    "    print(f\"Checking BIDS root directory: {bids_root_dir}\")\n",
    "    if not os.path.exists(bids_root_dir):\n",
    "        print(f\"Directory not found: {bids_root_dir}\")\n",
    "        return\n",
    "\n",
    "    # Define and check derivatives directory\n",
    "    derivatives_dir = os.path.expanduser('~/Documents/MRI/LEARN/BIDS_test/derivatives/physio/rest/')\n",
    "    print(f\"Checking derivatives directory: {derivatives_dir}\")\n",
    "    if not os.path.exists(derivatives_dir):\n",
    "        print(f\"Directory not found: {derivatives_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Define and check derivatives directory\n",
    "    derivatives_root_dir = os.path.expanduser('~/Documents/MRI/LEARN/BIDS_test/derivatives/physio/rest/')\n",
    "    print(f\"Checking derivatives directory: {derivatives_root_dir}\")\n",
    "    if not os.path.exists(derivatives_root_dir):\n",
    "        print(f\"Directory not found: {derivatives_root_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Define and check participants file\n",
    "    participants_file = os.path.join(bids_root_dir, 'participants.tsv')\n",
    "    print(f\"Checking participants file: {participants_file}\")\n",
    "    if not os.path.exists(participants_file):\n",
    "        print(f\"Participants file not found: {participants_file}\")\n",
    "        return\n",
    "\n",
    "    try: \n",
    "        # Read participants file and process groups\n",
    "        participants_df = pd.read_csv(participants_file, delimiter='\\t')\n",
    "        \n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(f\"Error reading participants file: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading participants file: {e}\")\n",
    "        return    \n",
    "    \n",
    "    print(f\"Found {len(participants_df)} participants\")\n",
    "    \n",
    "    #%% Start looping through participants by run\n",
    "#    for i, participant_id in enumerate(participants_df['participant_id']):\n",
    "    for i, participant_id in enumerate([participants_df['participant_id'].iloc[9]]):  # For testing  LRN010 first with ppg data\n",
    "\n",
    "        # Record the start time for this participant\n",
    "        participant_start_time = datetime.now()\n",
    "#        for run_number in range(1, 5):  # Assuming 4 runs\n",
    "        for run_number in range(1, 2):  # Testing with 1 run\n",
    "            try:\n",
    "        \n",
    "                # Set a higher DPI for better resolution\n",
    "                dpi_value = 300 \n",
    "\n",
    "                task_name = 'rest'\n",
    "\n",
    "                # Process the first run for the selected participant\n",
    "                run_id = f\"run-0{run_number}\"\n",
    "\n",
    "                # Construct the base path\n",
    "                base_path = os.path.join(derivatives_dir, 'ppg', participant_id, run_id)\n",
    "\n",
    "                # Make sure the directories exist\n",
    "                os.makedirs(base_path, exist_ok=True)\n",
    "                \n",
    "                # Define the processed signals filename for checking\n",
    "                processed_signals_filename = f\"{participant_id}_ses-1_task-rest_run-{run_number:02d}_physio_filtered_cleaned_ppg_processed.tsv.gz\"\n",
    "                processed_signals_path = os.path.join(base_path, processed_signals_filename)\n",
    "\n",
    "                # Check if processed file exists\n",
    "                if os.path.exists(processed_signals_path):\n",
    "                    print(f\"Processed PPG files found for {participant_id} for run {run_number}, skipping...\")\n",
    "                    continue  # Skip to the next run\n",
    "\n",
    "                # Setup logging\n",
    "                session_id = 'ses-1'  # Assuming session ID is known\n",
    "                \n",
    "                for handler in logging.root.handlers[:]:\n",
    "                    logging.root.removeHandler(handler)\n",
    "\n",
    "                log_file_path = setup_logging(participant_id, session_id, run_id, dataset_root_dir)\n",
    "                logging.info(f\"Testing PPG processing for task {task_name} run-0{run_number} for participant {participant_id}\")\n",
    "\n",
    "                sampling_rate = 5000    # acquisition sampling rate\n",
    "            \n",
    "                pattern = re.compile(f\"{participant_id}_ses-1_task-rest_run-{run_number:02d}_physio.tsv.gz\")\n",
    "                physio_files = [os.path.join(derivatives_dir, f) for f in os.listdir(derivatives_dir) if f'{participant_id}_ses-1_task-rest_run-{run_number:02d}_physio.tsv.gz' in f]\n",
    "                \n",
    "                bids_subject_dir = os.path.join(bids_root_dir, participant_id, session_id, 'func')\n",
    "                func_files = [os.path.join(bids_subject_dir, f) for f in os.listdir(bids_subject_dir) if f'{participant_id}_ses-1_task-rest_run-{run_number:02d}_bold.nii' in f]\n",
    "                \n",
    "                for file in physio_files:\n",
    "                    # Record the start time for this run\n",
    "                    run_start_time = datetime.now()\n",
    "                    logging.info(f\"Processing file: {file}\")\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # BIDS location for original 4D nifti data\n",
    "                        func_file = os.path.join(bids_subject_dir, f'{participant_id}_ses-1_task-rest_run-{run_number:02d}_bold.nii')\n",
    "                        original_nii_data_path = func_file\n",
    "                        logging.info(\"Original data path: {}\".format(original_nii_data_path))\n",
    "                        \n",
    "                        # Generate a base filename by removing the '.tsv.gz' extension\n",
    "                        base_filename_func = os.path.basename(file).replace('.nii', '')\n",
    "\n",
    "                        # Temp directory for intermediate files\n",
    "                        working_dir = os.path.expanduser('~/Documents/MRI/LEARN/BIDS_test/derivatives/physio/temp')     \n",
    "                        if not os.path.exists(working_dir):\n",
    "                            os.makedirs(working_dir)\n",
    "                        logging.info(\"Working directory: {}\".format(working_dir))\n",
    "                        \n",
    "                        # Output directory\n",
    "                        output_root_dir = os.path.expanduser('~/Documents/MRI/LEARN/BIDS_test/derivatives/physio/rest/_motion')         \n",
    "                        if not os.path.exists(output_root_dir):\n",
    "                            os.makedirs(output_root_dir)\n",
    "\n",
    "                        output_subject_dir = os.path.join(output_root_dir, participant_id, run_id)\n",
    "                        if not os.path.exists(output_subject_dir):\n",
    "                            os.makedirs(output_subject_dir)\n",
    "                        logging.info(\"Output subject directory: {}\".format(output_subject_dir))\n",
    "\n",
    "                        # Correctly forming the file name for the FD data\n",
    "                        fd_filename = f\"{participant_id}_{session_id}_task-{task_name}_{run_id}_framewise_displacement.tsv\"\n",
    "\n",
    "                        # Joining the output directory with the new file name\n",
    "                        fd_file_path = os.path.join(output_subject_dir, fd_filename)\n",
    "\n",
    "                        # Check if FD files already exist\n",
    "                        if not os.path.exists(fd_file_path):\n",
    "                            logging.info(f\"Processed FD files not found for {participant_id} for run {run_number}, proceeding with mcflirt...\")\n",
    "                            \n",
    "                            # Run MCFLIRT motion correction\n",
    "                            try:\n",
    "                                output_file = run_mcflirt_motion_correction(original_nii_data_path, output_subject_dir, working_dir)\n",
    "                                logging.info(f\"Motion corrected file saved at {output_file}\")\n",
    "                            except Exception as e:\n",
    "                                logging.error(f\"Failed to complete motion correction: {e}\")\n",
    "\n",
    "                            # Load motion parameters and calculate FD\n",
    "                            try:\n",
    "                                logging.info(\"Calculating framewise displacement...\")\n",
    "                                \n",
    "                                # Create FD tsv from mcflirt motion parameters\n",
    "                                motion_params_file = os.path.join(output_subject_dir, 'mcf_' + os.path.basename(original_nii_data_path) + '.par')\n",
    "                                fd = calculate_fd(motion_params_file)\n",
    "\n",
    "                                # Saving the FD data to the correct path\n",
    "                                save_fd_to_tsv(fd, output_subject_dir, fd_file_path)\n",
    "                                logging.info(f\"Framewise displacement data saved successfully to {fd_file_path}.\")\n",
    "\n",
    "                            except Exception as e:\n",
    "                                logging.error(\"Error in calculating framewise displacement: {}\".format(e))\n",
    "                                raise\n",
    "                        else: \n",
    "                            logging.info(f\"Processed FD files found for {participant_id} for run {run_number}, skipping mcflirt...\")\n",
    "                            \n",
    "                            # Read the existing FD data from the TSV\n",
    "                            try:\n",
    "                                fd = pd.read_csv(fd_file_path, delimiter='\\t')\n",
    "                                # Assuming the FD values are in the first column, if not, adjust the index accordingly.\n",
    "                                fd = fd.iloc[:, 0].values\n",
    "                            except Exception as e:\n",
    "                                logging.error(f\"Error in reading existing FD data: {e}\")\n",
    "                                raise   \n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error finding functional file for {participant_id} for run {run_number}: {e}\")\n",
    "                        return \n",
    "                    \n",
    "                    try:\n",
    "                        # Generate a base filename by removing the '.tsv.gz' extension\n",
    "                        base_filename = os.path.basename(file).replace('.tsv.gz', '')\n",
    "\n",
    "                        # Open the compressed PPG file and load it into a DataFrame\n",
    "                        with gzip.open(file, 'rt') as f:\n",
    "                            physio_data = pd.read_csv(f, delimiter='\\t')\n",
    "                            # Check for the presence of the 'ppg' column\n",
    "                            if 'ppg' not in physio_data.columns:\n",
    "                                logging.error(f\"'ppg' column not found in {file}. Skipping this file.\")\n",
    "                                continue  # Skip to the next file\n",
    "                            ppg = physio_data['ppg']\n",
    "\n",
    "                            # Calculate the time vector in minutes - Length of PPG data divided by the sampling rate gives time in seconds, convert to minutes\n",
    "                            time_vector = np.arange(len(ppg)) / sampling_rate / 60\n",
    "                            \n",
    "                            ### Step 1: Prefilter gradient artifact from PPG using comb-band stop filter at 1/TR (0.5 Hz) and downsample to 2000 Hz ###\n",
    "                            logging.info(f\"Step 1: Prefilter gradient artifact from PPG using comb-band stop filter at 1/TR (0.5 Hz) and downsample to 2000 Hz.\")\n",
    "\n",
    "                            # Pre-filter gradient artifact.\n",
    "                            ppg_filtered_array = comb_band_stop_filter(ppg, sampling_rate, visualize=False)\n",
    "\n",
    "                            # Downsample the filtered data.\n",
    "                            ppg_filtered_ds = nk.signal_resample(ppg_filtered_array, desired_length=None, sampling_rate=sampling_rate, desired_sampling_rate=100, method='pandas')\n",
    "                            ppg_unfiltered_ds = nk.signal_resample(ppg_filtered_array, desired_length=None, sampling_rate=sampling_rate, desired_sampling_rate=100, method='pandas')\n",
    "\n",
    "                            # Hanlde the index for the downsampled data\n",
    "                            new_length = len(ppg_filtered_ds)\n",
    "                            new_index = pd.RangeIndex(start=0, stop=new_length, step=1)  # or np.arange(new_length)\n",
    "\n",
    "                            # Convert the filtered data back into a Pandas Series\n",
    "                            ppg_filtered = pd.Series(ppg_filtered_ds, index=new_index)\n",
    "                            ppg_unfiltered = pd.Series(ppg_unfiltered_ds, index=new_index)\n",
    "\n",
    "                            if ppg_filtered.empty:\n",
    "                                logging.error(f\"Error: 'ppg_filtered' is empty.\")\n",
    "                                # Log stack trace for debugging purposes\n",
    "                                logging.error(traceback.format_exc())\n",
    "                            else:\n",
    "                                logging.info(f\"'ppg_filtered' is not empty, length: {len(ppg_filtered)}\")\n",
    "\n",
    "                            sampling_rate = 100    # downsampled sampling rate\n",
    "                            \n",
    "                            # Calculate the time vector in minutes - Length of PPG data divided by the sampling rate gives time in seconds, convert to minutes\n",
    "                            time_vector = np.arange(new_length) / sampling_rate / 60\n",
    "\n",
    "                            ### Step 2: Clean prefiltered PPG for non-default phasic decomposition and peak detection methods. ###\n",
    "                            logging.info(f\"Step 2: Clean prefiltered PPG for peak detection.\")\n",
    "\n",
    "                            # First, clean the PG signal\n",
    "                            ppg_cleaned = nk.ppg_clean(ppg_filtered, sampling_rate=sampling_rate, heart_rate=None, method=\"elgendi\") #\"nabian2018\"\n",
    "                            logging.info(f\"Prefiltered PPG signal cleaned using NeuroKit's ppg_clean.\")\n",
    "                            \n",
    "                            logging.info(f\"Starting peak detection for prefiltered cleaned PPG signal.\")\n",
    "                            logging.info(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "                            logging.info(f\"Size of prefiltered cleaned PPG signal: {ppg_cleaned.size}\")\n",
    "                            \n",
    "                            peak_methods = [\"elgendi\", \"bishop\"]\n",
    "                            logging.info(f\"Using the following methods for peak detection: {peak_methods}\")\n",
    "                            \n",
    "                            # Process FD\n",
    "                            fd_timeseries = fd  # Assuming 'fd' is your framewise displacement timeseries\n",
    "                            \n",
    "                             # Create a time array for the FD timeseries\n",
    "                            ppg_duration= len(ppg_cleaned) / sampling_rate  # Total duration in seconds\n",
    "                            fd_time_ppg = np.linspace(0, ppg_duration, len(fd_timeseries))\n",
    "                            \n",
    "                            # The new time array for the upsampled FD timeseries should match the PPG timeseries length\n",
    "                            # Make sure the last time point in upsampled_time does not exceed the last point in fd_time\n",
    "                            upsampled_time_ppg = np.linspace(0, ppg_duration, len(ppg_cleaned))\n",
    "                            \n",
    "                            # Use linear interpolation with bounds_error set to False to prevent extrapolation\n",
    "                            fd_interpolator_ppg = interp1d(fd_time_ppg, fd_timeseries, kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "                            fd_upsampled_ppg = fd_interpolator_ppg(upsampled_time_ppg)\n",
    "\n",
    "                            # Handle any NaN values that might have been introduced due to the bounds_error setting\n",
    "                            fd_upsampled_ppg[np.isnan(fd_upsampled_ppg)] = fd_timeseries[-1]  # Replace NaNs with the last valid FD value\n",
    "                            \n",
    "                            # Calculate FD-PPG correlation\n",
    "                            r_value_ppg, p_value_ppg = calculate_fd_ppg_correlation(fd_upsampled_ppg, ppg_cleaned)\n",
    "                            logging.info(f\"Correlation between FD and filtered cleaned PPG timeseries: {r_value_ppg}, p-value: {p_value_ppg}\")\n",
    "                            \n",
    "                            plot_filename = f\"{participant_id}_{session_id}_task-{task_name}_{run_id}_fd_ppg_correlation.png\"\n",
    "                            plot_filepath = os.path.join(base_path, plot_filename)\n",
    "                            plot_fd_ppg_correlation(fd_upsampled_ppg, ppg_cleaned, plot_filepath)\n",
    "                            logging.info(f\"FD-PPG correlation plot saved to {plot_filepath}\")\n",
    "\n",
    "                            # Assuming ppg_cleaned is a Pandas Series or a NumPy array\n",
    "                            ppg_cleaned_df = pd.DataFrame()\n",
    "                            ppg_cleaned_df['PPG_Unfiltered'] = ppg_unfiltered\n",
    "                            ppg_cleaned_df['PPG_Filtered'] = ppg_filtered\n",
    "                            ppg_cleaned_df['PPG_Clean'] = ppg_cleaned\n",
    "                            \n",
    "                            for peak_method in peak_methods:\n",
    "                                try:\n",
    "                                    # Detect peaks using the specified method\n",
    "                                    logging.info(f\"Detecting peaks using method: {peak_method}\")    \n",
    "                                    _, peaks = nk.ppg_peaks(ppg_cleaned, sampling_rate=sampling_rate, correct_artifacts=True, method=peak_method)\n",
    "                                    \n",
    "                                    # Log the detected R-Peaks for inspection\n",
    "                                    logging.info(f\"R Peaks via {peak_method}: {peaks['PPG_Peaks']}\")\n",
    "                                    \n",
    "                                    # Initialize the columns for PPG in the DataFrame\n",
    "                                    ppg_cleaned_df[f'PPG_Peaks_{peak_method}'] = 0\n",
    "\n",
    "                                    # Convert to 0-based indexing if your data is 1-based indexed\n",
    "                                    valid_peaks = peaks[f'PPG_Peaks'] - 1\n",
    "\n",
    "                                    # Update R Peaks, in the DataFrame\n",
    "                                    ppg_cleaned_df.loc[valid_peaks, f'PPG_Peaks_{peak_method}'] = 1\n",
    "                                    \n",
    "                                    voxel_threshold = 0.5 # mm\n",
    "                                    # Calculate the number of volumes (assuming 2 sec TR and given sampling rate)\n",
    "                                    num_volumes = len(fd_upsampled_ppg) / (sampling_rate * 2)\n",
    "\n",
    "                                    # Generate the volume numbers for the x-axis\n",
    "                                    volume_numbers = np.arange(0, num_volumes)\n",
    "                                    #%% Plotly subplots\n",
    "                                    # Create a plotly figure with subplots\n",
    "                                    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, shared_yaxes=False, subplot_titles=('Filtered Raw and Cleaned PPG Signal', f'PPG with {peak_method} R Peaks', '', 'Framewise Displacement'), vertical_spacing=0.05)\n",
    "\n",
    "                                    # Plot 1: Overlay of Raw and Cleaned PPG\n",
    "                                    fig.add_trace(go.Scatter(y=ppg_unfiltered, mode='lines', name='Raw Unfiltered PPG', line=dict(color='red', width=1, dash='dash')), row=1, col=1)\n",
    "                                    fig.add_trace(go.Scatter(y=ppg_filtered, mode='lines', name='Raw Filtered PPG', line=dict(color='blue', width=1, dash='dot')), row=1, col=1)\n",
    "                                    fig.add_trace(go.Scatter(y=ppg_cleaned, mode='lines', name='Filtered Cleaned PPG', line=dict(color='green', width=2)), row=1, col=1) #opacity=0.5\n",
    "                                    \n",
    "                                    # Plot 2: PPG timeseries with R-Peak Events\n",
    "                                    fig.add_trace(go.Scatter(y=ppg_cleaned, mode='lines', name='Filtered Cleaned PPG', line=dict(color='green')), row=2, col=1)\n",
    "\n",
    "                                    # Ensure valid_peaks are within the range of the DataFrame\n",
    "                                    valid_peaks = valid_peaks[valid_peaks < len(ppg_cleaned_df)]\n",
    "                                    y_values = ppg_cleaned_df.loc[valid_peaks, 'PPG_Clean']\n",
    "\n",
    "                                    # Scatter Plot for R Peaks\n",
    "                                    fig.add_trace(go.Scatter(x=valid_peaks, y=y_values, mode='markers', name='R Peaks', marker=dict(color='red')), row=2, col=1)\n",
    "\n",
    "                                    # Plot 3: Framewise Displacement\n",
    "                                    fig.add_trace(go.Scatter(y=fd_upsampled_ppg, mode='lines', name='Framewise Displacement', line=dict(color='blue')), row=3, col=1)\n",
    "                                    # Add a horizontal line for the voxel_threshold\n",
    "                                    fig.add_hline(y=voxel_threshold, line=dict(color='red', dash='dash'), row=3, col=1)\n",
    "\n",
    "                                    # Update x-axis and y-axis labels\n",
    "                                    fig.update_xaxes(title_text='Samples', row=1, col=1)\n",
    "                                    fig.update_yaxes(title_text='Amplitude (Volts)', row=1, col=1)\n",
    "                                    fig.update_yaxes(title_text='Amplitude (Volts)', row=2, col=1)\n",
    "                                    fig.update_xaxes(title_text='Samples', row=2, col=1)\n",
    "                                    fig.update_yaxes(title_text='FD (mm)', row=3, col=1)\n",
    "\n",
    "                                    # Update layout and size\n",
    "                                    #fig.update_layout(height=800, width=1000, title_text=f'PPG Analysis - {peak_method}')\n",
    "\n",
    "                                    # Update layout and size\n",
    "                                    fig.update_layout(height=1200, width=1800, title_text=f'PPG Analysis - {peak_method}')\n",
    "\n",
    "                                    # Disable y-axis zooming for all subplots\n",
    "                                    fig.update_yaxes(fixedrange=True)\n",
    "\n",
    "                                    # Calculate the tick positions\n",
    "                                    tick_interval = 10  # Adjust this value as needed\n",
    "                                    tick_positions = np.arange(0, len(fd_upsampled_ppg), tick_interval * sampling_rate * 2)\n",
    "\n",
    "                                    # Create the tick labels\n",
    "                                    tick_labels = [f\"{int(vol)}\" for vol in volume_numbers[::tick_interval]]\n",
    "\n",
    "                                    # Update the third subplot's x-axis with these tick positions and labels\n",
    "                                    fig.update_xaxes(\n",
    "                                        title_text='Volume Number (2 sec TR)',\n",
    "                                        tickvals=tick_positions,\n",
    "                                        ticktext=tick_labels,\n",
    "                                        row=3, col=1\n",
    "                                    )\n",
    "\n",
    "                                    # Update the third subplot's y-axis title\n",
    "                                    fig.update_yaxes(\n",
    "                                        title_text='FD (mm)',\n",
    "                                        row=3, col=1\n",
    "                                    )\n",
    "\n",
    "                                    # Update the third subplot's title\n",
    "                                    #fig['layout']['annotations'][2]['text'] = 'Framewise Displacement'  # This assumes that the third annotation is for the third subplot\n",
    "\n",
    "                                    combo_plot_filename = os.path.join(base_path, f\"{base_filename}_filtered_cleaned_ppg_{peak_method}_subplots.svg\")\n",
    "                                    \n",
    "                                    # Save the plot as an HTML file\n",
    "                                    plotly.offline.plot(fig, filename=f\"{combo_plot_filename}_filtered_cleaned_ppg_{peak_method}_subplots.html\")\n",
    "\n",
    "                                    # Display the figure\n",
    "                                    fig.show()\n",
    "#                                   %% Matplotlib subplots\n",
    "                                    # Create and configure matplotlib svg plots\n",
    "                                    fig, axes = plt.subplots(3, 1, figsize=(20, 10), sharex=True)\n",
    "\n",
    "                                    logging.info(f\"Unfiltered PPG range: {ppg_unfiltered.min()}, {ppg_unfiltered.max()}\")\n",
    "                                    logging.info(f\"Filtered PPG range: {ppg_filtered.min()}, {ppg_filtered.max()}\")\n",
    "                                    logging.info(f\"Cleaned PPG range: {ppg_cleaned.min()}, {ppg_cleaned.max()}\")\n",
    "\n",
    "                                    # Plot 1: Overlay of Raw and Cleaned PPG\n",
    "                                    axes[0].plot(ppg_unfiltered, label='Raw Unfiltered PPG', color='blue', linewidth=1)\n",
    "                                    axes[0].plot(ppg_filtered, label='Raw Filtered PPG', color='green', linewidth=1)\n",
    "                                    axes[0].plot(ppg_cleaned, label='Filtered Cleaned PPG', color='orange', linewidth=1)\n",
    "                                    #axes[0].plot(ppg_unfiltered, label='Raw Unfiltered PPG', linestyle='--')\n",
    "                                    #axes[0].plot(ppg_filtered, label='Raw Filtered PPG', linestyle='-.')\n",
    "\n",
    "                                    axes[0].set_title('Filtered Raw and Cleaned PPG Signal')\n",
    "                                    axes[0].set_xlabel(f'Samples ({sampling_rate} Hz)')\n",
    "                                    axes[0].set_ylabel('Amplitude (Volts)')\n",
    "                                    axes[0].legend()\n",
    "\n",
    "                                    # Plot 2: PPG timeseries with R-Peak Events\n",
    "                                    axes[1].plot(ppg_cleaned, label='PPG', color='green')\n",
    "                                    \n",
    "                                    # Ensure valid_peaks are within the range of the DataFrame\n",
    "                                    valid_peaks = valid_peaks[valid_peaks < len(ppg_cleaned_df)]\n",
    "\n",
    "                                    # Scatter Plot for R Peaks\n",
    "                                    y_values = ppg_cleaned_df.loc[valid_peaks, 'PPG_Clean']\n",
    "                                    axes[1].scatter(valid_peaks, y_values, color='red', label='R Peaks')\n",
    "\n",
    "                                    # Add legend and set titles\n",
    "                                    axes[1].set_title(f'PPG with {peak_method} R Peaks')\n",
    "                                    axes[1].set_xlabel(f'Samples ({sampling_rate} Hz)')\n",
    "                                    axes[1].set_ylabel('Amplitude (Volts)')\n",
    "                                    axes[1].legend()\n",
    "\n",
    "                                    # Plot 3: Framewise Displacement\n",
    "                                    axes[2].plot(fd_upsampled_ppg, label='Framewise Displacement', color='blue')\n",
    "                                    axes[2].axhline(y=voxel_threshold, color='r', linestyle='--')\n",
    "\n",
    "                                    # Set x-axis ticks to display volume numbers at regular intervals\n",
    "                                    # The interval for ticks can be adjusted (e.g., every 10 volumes)\n",
    "                                    tick_interval = 10  # Adjust this value as needed\n",
    "                                    axes[2].set_xticks(np.arange(0, len(fd_upsampled_ppg), tick_interval * sampling_rate * 2))\n",
    "                                    axes[2].set_xticklabels([f\"{int(vol)}\" for vol in volume_numbers[::tick_interval]])\n",
    "\n",
    "                                    axes[2].set_title('Framewise Displacement')\n",
    "                                    axes[2].set_xlabel('Volume Number (2 sec TR)')\n",
    "                                    axes[2].set_ylabel('FD (mm)')\n",
    "\n",
    "                                    # # Add shading where FD is above threshold across all subplots\n",
    "                                    # for ax in axes[:-1]: # Exclude the last axis which is for FD plot\n",
    "                                    #     ax.fill_between(ppg_cleaned.index / sampling_rate / 60, 0, voxel_threshold, where=fd_upsampled_phasic > voxel_threshold, color='red', alpha=0.3)\n",
    "\n",
    "                                    # Save the combined plot\n",
    "                                    plt.tight_layout()\n",
    "                                    combo_plot_filename = os.path.join(base_path, f\"{base_filename}_filtered_cleaned_ppg_{peak_method}_subplots.png\")\n",
    "                                    plt.savefig(combo_plot_filename, dpi=dpi_value)\n",
    "                                    plt.show()\n",
    "                                    #plt.close()\n",
    "                                    logging.info(f\"Saved PPG subplots for with {peak_method} to {combo_plot_filename}\")\n",
    "                                            #%% PSD Plotly Plots\n",
    "                                    # Compute Power Spectral Density 0 - 100 Hz for PPG\n",
    "                                    logging.info(f\"Computing Power Spectral Density (PSD) for filtered PPG using multitapers hann windowing.\")\n",
    "                                    ppg_filtered_psd = nk.signal_psd(ppg_cleaned_df['PPG_Clean'], sampling_rate=sampling_rate, method='multitapers', show=False, normalize=True, \n",
    "                                                        min_frequency=0, max_frequency=100.0, window=None, window_type='hann',\n",
    "                                                        silent=False, t=None)\n",
    "\n",
    "                                    # Plotting Power Spectral Density\n",
    "                                    logging.info(f\"Plotting Power Spectral Density (PSD) 0 - 100 Hz for filtered cleaned PPG using multitapers hann windowing.\")\n",
    "\n",
    "                                    # Create a Plotly figure\n",
    "                                    fig = go.Figure()\n",
    "\n",
    "                                    # Adding the Power Spectral Density trace\n",
    "                                    fig.add_trace(go.Scatter(x=ppg_filtered_psd['Frequency'], y=ppg_filtered_psd['Power'],\n",
    "                                                            mode='lines', name='Normalized PPG PSD',\n",
    "                                                            line=dict(color='blue'), fill='tozeroy'))\n",
    "\n",
    "                                    # Update layout for the plot\n",
    "                                    fig.update_layout(title=f'Power Spectral Density (PSD) (Multitapers with Hanning Window) for Filtered Cleaned PPG',\n",
    "                                                    xaxis_title='Frequency (Hz)',\n",
    "                                                    yaxis_title='Normalized Power',\n",
    "                                                    template='plotly_white',\n",
    "                                                    width=1200, height=600)\n",
    "\n",
    "                                    # Save the plot as an SVG file\n",
    "                                    plot_filename = os.path.join(base_path, f\"{base_filename}_filtered_cleaned_ppg_psd.svg\")\n",
    "                                    pyo.plot(fig, filename=plot_filename, image='svg', image_filename=plot_filename, auto_open=False)\n",
    "\n",
    "                                    # Display the figure\n",
    "                                    fig.show()\n",
    "                                    #%% Matplotlib PSD Plots\n",
    "                                    # Plotting Power Spectral Density\n",
    "                                    logging.info(f\"Plotting Power Spectral Density (PSD) 0 - 100 Hz for filtered cleaned PPG using multitapers hann windowing.\")\n",
    "                                    plt.figure(figsize=(12, 6))\n",
    "                                    plt.fill_between(ppg_filtered_psd['Frequency'], ppg_filtered_psd['Power'], color='blue', alpha=0.3)  # alpha controls the transparency\n",
    "                                    plt.plot(ppg_filtered_psd['Frequency'], ppg_filtered_psd['Power'], color='blue', label='Normalized PPG PSD (Multitapers with Hanning Window)')\n",
    "                                    plt.title(f'Power Spectral Density (PSD) (Multitapers with Hanning Window) for filtered cleaned PPG')\n",
    "                                    plt.xlabel('Frequency (Hz)')\n",
    "                                    plt.ylabel('Normalized Power')\n",
    "                                    plt.legend()\n",
    "                                    plot_filename = os.path.join(base_path, f\"{base_filename}_filtered_cleaned_ppg_psd.png\")\n",
    "                                    plt.savefig(plot_filename, dpi=dpi_value)\n",
    "                                    plt.show()\n",
    "                                    plt.close()\n",
    "                                \n",
    "                                    # Create a mask for FD values less than or equal to 0.5\n",
    "                                    mask_ppg = fd_upsampled_ppg <= 0.5\n",
    "\n",
    "                                    # Apply the mask to both FD and PPG data\n",
    "                                    filtered_fd_ppg = fd_upsampled_ppg[mask_ppg]\n",
    "                                    filtered_ppg = ppg_cleaned[mask_ppg]\n",
    "\n",
    "                                    # Initialize correlation and above threshold FD variables as NaN\n",
    "                                    r_value_ppg = np.nan\n",
    "                                    p_value_ppg = np.nan\n",
    "                                    r_value_ppg_thresh = np.nan\n",
    "                                    p_value_ppg_thresh = np.nan\n",
    "\n",
    "                                    num_samples_above_threshold = np.nan\n",
    "                                    percent_samples_above_threshold = np.nan\n",
    "                                    mean_fd_above_threshold = np.nan\n",
    "                                    std_dev_fd_above_threshold = np.nan\n",
    "                                    \n",
    "                                    # Check if there are any FD values above the threshold\n",
    "                                    if np.any(fd > voxel_threshold):\n",
    "                                        # Check if filtered data is not empty\n",
    "                                        if len(filtered_fd_ppg) > 0 and len(filtered_ppg) > 0:\n",
    "                                            \n",
    "                                            # Calculate above threshold FD statistics\n",
    "                                            num_samples_above_threshold = np.sum(fd_upsampled_ppg > voxel_threshold)\n",
    "                                            percent_samples_above_threshold = num_samples_above_threshold / len(fd_upsampled_ppg) * 100\n",
    "                                            mean_fd_above_threshold = np.mean(fd_upsampled_ppg[fd_upsampled_ppg > voxel_threshold]) if num_samples_above_threshold > 0 else np.nan\n",
    "                                            std_dev_fd_above_threshold = np.std(fd_upsampled_ppg[fd_upsampled_ppg > voxel_threshold]) if num_samples_above_threshold > 0 else np.nan\n",
    "                                            \n",
    "                                            # Calculate the correlation between filtered FD and PPG\n",
    "                                            r_value_ppg, p_value_ppg = calculate_fd_ppg_correlation(filtered_fd_ppg, filtered_ppg)\n",
    "                                            logging.info(f\"Correlation between FD (filtered) and filtered cleaned PPG timeseries < {voxel_threshold} mm: {r_value_ppg}, p-value: {p_value_ppg}\")\n",
    "\n",
    "                                            plot_filename = f\"{participant_id}_{session_id}_task-{task_name}_{run_id}_fd_ppg_correlation_filtered.svg\"\n",
    "                                            plot_filepath = os.path.join(base_path, plot_filename)\n",
    "                                            plot_fd_ppg_correlation(fd_upsampled_ppg, filtered_ppg, plot_filepath)\n",
    "                                            logging.info(f\"FD-PPG filtered correlation plot saved to {plot_filepath}\")\n",
    "                                            \n",
    "                                        else:\n",
    "                                            # Log a warning if there are no FD values below the threshold after filtering\n",
    "                                            logging.warning(f\"No FD values below {voxel_threshold} mm. Correlation cannot be calculated.\")\n",
    "                                    else:\n",
    "                                        # Log a warning if there are no FD values above the threshold\n",
    "                                        logging.warning(f\"No FD values above {voxel_threshold} mm. No need to filter and calculate correlation.\")\n",
    "\n",
    "                                     # Calculate statistics related to framewise displacement\n",
    "                                    mean_fd_below_threshold = np.mean(fd_upsampled_ppg[fd_upsampled_ppg < voxel_threshold])\n",
    "                                    std_dev_fd_below_threshold = np.std(fd_upsampled_ppg[fd_upsampled_ppg < voxel_threshold])\n",
    "\n",
    "                                    # Average SCR Frequency (counts/min)\n",
    "                                    total_time_minutes = len(ppg_cleaned) / (sampling_rate * 60)\n",
    "                                    average_ppg_frequency = len(valid_peaks) / total_time_minutes\n",
    "\n",
    "                                    # Average, Std Deviation, Max, Min Inter-SCR Interval (sec)\n",
    "                                    inter_ppg_intervals = np.diff(valid_peaks) / sampling_rate\n",
    "                                    average_inter_ppg_interval = inter_ppg_intervals.mean()\n",
    "                                    std_inter_ppg_interval = inter_ppg_intervals.std()\n",
    "                                    max_inter_ppg_interval = inter_ppg_intervals.max()\n",
    "                                    min_inter_ppg_interval = inter_ppg_intervals.min()\n",
    "\n",
    "                                    # Update the scr_stats dictionary\n",
    "                                    ppg_stats = {\n",
    "                                        'R-Peak Count (# peaks)': len(valid_peaks),\n",
    "                                        'Average R-Peak Frequency (counts/min)': average_ppg_frequency,\n",
    "                                        'Average Inter-Peak Interval (sec)': average_inter_ppg_interval,\n",
    "                                        'Std Deviation Inter-Peak Interval (sec)': std_inter_ppg_interval,\n",
    "                                        'Max Inter-Peak Interval (sec)': max_inter_ppg_interval,\n",
    "                                        'Min Inter-Peak Interval (sec)': min_inter_ppg_interval,\n",
    "                                        'Mean Framewise Displacement (mm)': fd_upsampled_ppg.mean(),\n",
    "                                        'Std Deviation Framewise Displacement (mm)': fd_upsampled_ppg.std(),\n",
    "                                        'Max Framewise Displacement (mm)': fd_upsampled_ppg.max(),\n",
    "                                        'Min Framewise Displacement (mm)': fd_upsampled_ppg.min(),\n",
    "                                        'Number of samples with FD > 0.5 mm': num_samples_above_threshold,\n",
    "                                        'Percent of samples with FD > 0.5 mm': percent_samples_above_threshold,\n",
    "                                        'Mean FD > 0.5 mm': mean_fd_above_threshold,\n",
    "                                        'Std Deviation FD > 0.5 mm': std_dev_fd_above_threshold,\n",
    "                                        'Mean FD < 0.5 mm': mean_fd_below_threshold,\n",
    "                                        'Std Deviation FD < 0.5 mm': std_dev_fd_below_threshold,\n",
    "                                        'Framewise Displacement - PPG Correlation R-Value': r_value_ppg,\n",
    "                                        'Framewise Displacement - PPG Correlation P-Value': p_value_ppg,\n",
    "                                        'Framewise Displacement - PPG Correlation R-Value (FD < 0.5 mm)': r_value_ppg_thresh,\n",
    "                                        'Framewise Displacement - PPG Correlation P-Value (FD < 0.5 mm)': p_value_ppg_thresh,\n",
    "                                        }\n",
    "                                        \n",
    "                                    # Debug: Check the updated PPG statistics\n",
    "                                    logging.info(f\"PPG Stats: {ppg_stats}\")\n",
    "\n",
    "                                    # Assume ppg_stats are dictionaries containing the statistics\n",
    "                                    ppg_stats_df = pd.DataFrame(ppg_stats.items(), columns=['Statistic', 'Value'])\n",
    "\n",
    "                                    # Concatenate DataFrames vertically, ensuring the order is maintained\n",
    "                                    ppg_summary_stats_df = pd.concat([ppg_stats_df], axis=0, ignore_index=True)\n",
    "                                    \n",
    "                                    # Add a column to indicate the category of each statistic\n",
    "                                    ppg_summary_stats_df.insert(0, 'Category', '')\n",
    "                                    ppg_summary_stats_df.loc[:len(ppg_stats)-1, 'Category'] = 'PPG Stats'\n",
    "\n",
    "                                    # Save the summary statistics to a TSV file, with headers and without the index\n",
    "                                    summary_stats_filename = os.path.join(base_path, f\"{base_filename}_filtered_cleaned_ppg_{peak_method}_summary_statistics.tsv\")\n",
    "                                    ppg_summary_stats_df.to_csv(summary_stats_filename, sep='\\t', header=True, index=False)\n",
    "                                    logging.info(f\"Saving summary statistics to TSV file: {summary_stats_filename}\")\n",
    "                                \n",
    "                                except Exception as e:\n",
    "                                    logging.error(f\"Error in peak detection {peak_method}): {e}\")\n",
    "                                    # Log stack trace for debugging purposes\n",
    "                                    logging.error(traceback.format_exc())\n",
    "                                    continue\n",
    "\n",
    "                                # Save ppg_cleaned data to a TSV file\n",
    "                                logging.info(f\"Saving ppg_cleaned data to TSV file.\")\n",
    "                                ppg_cleaned_df_filename = os.path.join(base_path, f\"{base_filename}_filtered_cleaned_ppg_processed.tsv\")\n",
    "                                ppg_cleaned_df['FD_Upsampled'] = fd_upsampled_ppg\n",
    "                                ppg_cleaned_df.to_csv(ppg_cleaned_df_filename, sep='\\t', index=False)\n",
    "                                \n",
    "                                # Compress the TSV file\n",
    "                                with open(ppg_cleaned_df_filename, 'rb') as f_in:\n",
    "                                    with gzip.open(f\"{ppg_cleaned_df_filename}.gz\", 'wb') as f_out:\n",
    "                                        f_out.writelines(f_in)\n",
    "                                \n",
    "                                # Remove the uncompressed file\n",
    "                                os.remove(ppg_cleaned_df_filename)\n",
    "                                logging.info(f\"Saved and compressed ppg_cleaned data to {ppg_cleaned_df_filename}.gz\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing ppg file {file}: {e}\")\n",
    "                        traceback.print_exc()\n",
    "                        traceback_info = traceback.format_exc()\n",
    "                        logging.error(f\"Traceback Information: \\n{traceback_info}\")\n",
    "                        return \n",
    "                        #continue # Continue to the next file\n",
    "                \n",
    "                # Remove the working directory\n",
    "                try:\n",
    "                    shutil.rmtree(working_dir)\n",
    "                    logging.info(\"Working directory removed successfully.\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error removing working directory: {e}\")\n",
    "                    raise e\n",
    "                \n",
    "                # Record the end time for this run and calculate runtime\n",
    "                run_end_time = datetime.now()\n",
    "                run_runtime = (run_end_time - run_start_time).total_seconds() / 60\n",
    "                logging.info(f\"Run {run_number} completed. Runtime: {run_runtime} minutes.\")\n",
    "                log_resource_usage()  # Log resource usage at the end of each run\n",
    "            \n",
    "            except Exception as e:\n",
    "            # Log the error\n",
    "                logging.error(f\"Error processing run {run_number} for participant {participant_id}: {e}\")\n",
    "                traceback.print_exc()  # Print the traceback for debugging purposes\n",
    "                traceback_info = traceback.format_exc()\n",
    "                logging.error(f\"Traceback Information: \\n{traceback_info}\")\n",
    "                continue  # Continue to the next run\n",
    "        \n",
    "        # Record the end time for this participant and calculate runtime\n",
    "        participant_end_time = datetime.now()\n",
    "        participant_runtime = (participant_end_time - participant_start_time).total_seconds() / 60\n",
    "        print(f\"Participant {participant_id} completed. Runtime: {participant_runtime} minutes.\")\n",
    "                \n",
    "    # Record the script end time and calculate runtime\n",
    "    end_time = datetime.now()\n",
    "    script_runtime = (end_time - start_time).total_seconds() / 60\n",
    "    print(f\"Main function completed. Script runtime: {script_runtime} minutes. Processing PPG complete for participant {participant_id}.\")\n",
    "        \n",
    "#%% Main script function\n",
    "# If this script is run as the main script, execute the main function.\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # # Call the main function.\n",
    "    # cProfile.run('main()')\n",
    "\n",
    "    main()\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
